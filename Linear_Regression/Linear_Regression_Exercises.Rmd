---
title: 'Linear Regression: Exercises'
author: "Kateřina Večerková vecerkok@vscht.cz"
date: "2024-11-28"
output: html_document
---

# Exercises

1. **Covariance and Correlation Calculation**
   - **Exercise 1:**  
     Given the following data for variables `x` and `y`, calculate the covariance and correlation by hand (using the formulas) and compare the results with the values calculated in R.
```{r}
x <- c(1, 3, 5, 7, 9)
y <- c(2, 4, 6, 8, 10)

# TODO your code here
```
   
   - **Exercise 2:**  
     Consider a different set of data, `x = c(1, 4, 7, 10, 13)` and `y = c(13, 10, 7, 4, 1)`.  
     Calculate the covariance and correlation and interpret the results. What does the negative correlation suggest about the relationship between `x` and `y`?

```{r}
# TODO your code here
```

2. **Linear Regression Model on the `mtcars` Dataset**
   - **Exercise 3:**  
     Use the `mtcars` dataset to fit a **simple linear regression** model predicting `mpg` based on `hp` (horsepower).  
     - Plot the data and add the regression line to the plot.
     - Interpret the slope and intercept of the model. What do they mean in the context of the data?

```{r}
# TODO your code here
```

   - **Exercise 4:**  
     Fit a **multiple linear regression** model using `mpg` as the dependent variable and `hp`, `wt`, and `qsec` as independent variables.  
     - Check the summary of the model. Which variable has the strongest effect on `mpg`?  
     - Plot the residuals of the model and interpret whether they seem homoscedastic (constant variance).

```{r}
# TODO your code here
```

3. **Multicollinearity Check**
   - **Exercise 5:**  
     Check for multicollinearity using the `vif()` function on the multiple linear regression model from Exercise 4.  
     - What do the VIF values suggest? If any predictors have high VIF values, what should you do next? 
     - Create a correlation matrix of the predictors and visualize it using a heatmap (you can use the `ggcorrplot` function).
   
```{r}
# TODO your code here
```

4. **Model Evaluation**
   - **Exercise 6:**  
     Evaluate the **R-squared**, **Adjusted R-squared**, **RMSE**, and **MAE** for the multiple linear regression model from Exercise 4.  
     - Which of these metrics gives you the best understanding of how well the model fits the data?

```{r}
# TODO your code here
```

5. **Model Comparison with ANOVA**
   - **Exercise 7:**  
     Fit a second multiple linear regression model excluding the `disp` and `cyl` variables from the first model to avoid multicollinearity.  
     - Use ANOVA to compare the two models and interpret whether removing these variables improved the model.
   
```{r}
# TODO your code here
```

### Discussion Questions

1. **Interpreting Correlation**  
   If you calculate a correlation of -0.95 between two variables, what does that imply about their relationship? Can this be used for prediction?

2. **Effect of Outliers on Linear Regression**  
   How might outliers affect the results of a linear regression model? Can you suggest ways to handle outliers in your data?

3. **Checking for Homoscedasticity**  
   Why is homoscedasticity important in linear regression, and how can you detect it? What might happen if this assumption is violated?

4. **Overfitting vs. Underfitting**  
   What are the consequences of overfitting and underfitting a linear regression model? How can you address each problem?

### Extra Challenges

1. **Polynomial Regression**
   - Extend the linear regression model by fitting a **polynomial regression** model (e.g., quadratic or cubic) and compare the results to the linear model.  
     - Does the polynomial model provide a better fit? Justify your answer.

```{r}
# TODO your code here
```

2. **Ridge and Lasso Regression**
   - Explore **Ridge** and **Lasso** regression using the `glmnet` package to prevent overfitting.  
     - Compare the coefficients of the Ridge and Lasso models to the standard linear regression model.

```{r}
# TODO your code here
```
