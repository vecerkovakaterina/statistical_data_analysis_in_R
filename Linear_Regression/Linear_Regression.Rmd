---
title: "Linear Regression"
author: "Kateřina Večerková vecerkok@vscht.cz"
date: "2024-11-28"
output: html_document
---

# **Covariance**

Covariance measures how two variables change together. It shows the direction of their relationship:

- **Positive covariance** means both variables increase together.  
- **Negative covariance** means as one increases, the other decreases.  
- **Zero covariance** suggests no linear relationship.

### **Formulas**

1. **Sample Covariance**:
\[
\text{Cov}(X, Y) = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{n - 1}
\]
- \( X_i, Y_i \): Individual data points of variables \( X \) and \( Y \).  
- \( \bar{X}, \bar{Y} \): Means of \( X \) and \( Y \).  
- \( n \): Sample size.

2. **Population Covariance**:
\[
\text{Cov}(X, Y) = \frac{\sum (X_i - \mu_X)(Y_i - \mu_Y)}{N}
\]
- \( \mu_X, \mu_Y \): Population means of \( X \) and \( Y \).  
- \( N \): Population size.

```{r}
x <- c(2, 4, 6, 8)
y <- c(1, 3, 5, 7)

covariance <- cov(x, y)
print(paste("Covariance:", covariance))
```

# **Correlation**

Correlation normalizes covariance, measuring the strength and direction of the relationship between two variables, ranging from -1 to 1.

- **1**: Perfect positive relationship  
- **0**: No linear relationship  
- **-1**: Perfect negative relationship

### **Formulas**

1. **Sample Correlation Coefficient**:
\[
r = \frac{\text{Cov}(X, Y)}{s_X \cdot s_Y}
\]
- \( s_X, s_Y \): Sample standard deviations of \( X \) and \( Y \).

2. **Population Correlation Coefficient**:
\[
\rho = \frac{\text{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y}
\]
- \( \sigma_X, \sigma_Y \): Population standard deviations of \( X \) and \( Y \).

```{r}
x <- c(2, 4, 6, 8)
y <- c(1, 3, 5, 7)

correlation <- cor(x, y)
print(paste("Correlation:", correlation))
```

# **Linear Regression**

Linear regression models the relationship between a dependent variable \( Y \) and one or more independent variables \( X \).

### **Formula**
1. **Population Regression Line**:
\[
Y = \beta_0 + \beta_1 X + \epsilon
\]
- \( \beta_0 \): Population intercept.  
- \( \beta_1 \): Population slope.  
- \( \epsilon \): Residual error.

2. **Sample Regression Line**:
\[
\hat{Y} = b_0 + b_1 X
\]
- \( b_0 \): Sample intercept, estimate of \( \beta_0 \).  
- \( b_1 \): Sample slope, estimate of \( \beta_1 \).

### **Types of Linear Regression**

1. **Simple Linear Regression**: One independent variable.
```{r}
model <- lm(mpg ~ hp, data = mtcars)
summary(model)
plot(mtcars$hp, mtcars$mpg)
abline(model, col = "red")
```

2. **Multiple Linear Regression**: Multiple independent variables.
```{r}
model_mult <- lm(mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb, data = mtcars)
summary(model_mult)
```

```{r}
# Plotting residuals
model_residuals <- model_mult$residuals
hist(model_residuals)
```

```{r}
# Checking normality with a Q-Q plot
qqnorm(model_residuals)
qqline(model_residuals)
```

### Checking Multicollinearity
```{r}
library(car)
vif(model_mult)
```

```{r}
# Install and load the ggcorrplot package
library(ggcorrplot)

# Remove the Customer Value column
reduced_data <- subset(mtcars, select = -mpg)

# Compute correlation at 2 decimal places
corr_matrix = round(cor(reduced_data), 2)

# Compute and show the  result
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower",
          lab = TRUE)
```

We notice three strong correlations because their value is higher than 0.8. Let's build a second model without the highly correlated disp and cyl variables to avoid multicollinearity.

```{r}
second_model_mult <- lm(mpg ~ hp + drat + wt + qsec + vs + am + gear + carb, data = mtcars)
summary(second_model_mult)
```

### Model Comparison

To determine which model is better, we can use ANOVA, where the null hypothesis is that the variables that we removed previously have no significance against the alternative hypothesis that they were significant. If the new model is an improvement of the original model, then we fail to reject H0.

```{r}
anova(second_model_mult, model_mult)
```

## **Assumptions of Linear Regression**

1. **Linearity**: The relationship between predictors and the response variable is linear. This linearity can be visually inspected using scatterplots, which should reveal a straight-line relationship rather than a curvilinear one.
2. **Independence**: Observations are independent of each other. This has to be checked on the experiment design.
3. **Homoscedasticity**: The variance of residuals (the differences between observed and predicted values) is constant across all levels of the independent variables. A scatterplot of residuals versus predicted values should not display any discernible pattern, such as a cone-shaped distribution.
4. **Normality of Residuals**: Residuals are normally distributed. Can be checked by examining histograms or QQ plots of the residuals or with a formal normality test.
5. **No Multicollinearity**: Predictors are not highly correlated with each other. Can be checked with a correlation matrix, where correlation coefficients should ideally be below 0.80.

## **Model Evaluation Metrics**

To assess the quality of a regression model, several metrics are used:

### 1. Coefficient of Determination (\( R^2 \)):
Measures the proportion of the variance in the dependent variable explained by the independent variables.
\[
R^2 = 1 - \frac{\text{Sum of Squared Residuals (SSR)}}{\text{Total Sum of Squares (SST)}}
\]

- \( R^2 = 1 \): Perfect fit.
- \( R^2 = 0 \): Model explains none of the variance.

### 2. Adjusted \( R^2 \):
Adjusted for the number of predictors, penalizing overfitting.
\[
R_{\text{adj}}^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}
\]
Where \( n \) is the number of observations and \( p \) is the number of predictors.

### 3. Root Mean Squared Error (RMSE):
Measures the average magnitude of residuals.
\[
\text{RMSE} = \sqrt{\frac{\sum (Y_{\text{observed}} - Y_{\text{predicted}})^2}{n}}
\]

### 4. Mean Absolute Error (MAE):
Measures the average absolute residuals.
\[
\text{MAE} = \frac{\sum |Y_{\text{observed}} - Y_{\text{predicted}}|}{n}
\]

## **Key Concepts**

1. **Coefficients**: Represent the change in \( Y \) for a unit change in \( X \).
2. **Residuals**: Differences between observed and predicted values.
3. **Multicollinearity**: When predictors are highly correlated, affecting the model's stability.
4. **Interaction Effects**: When the combined effect of two variables is different from their individual effects.

## **Common Pitfalls**

- **Overfitting**: Too many predictors lead to poor performance on new data.
- **Underfitting**: Too few predictors miss important relationships.
- **Outliers**: Extreme values can distort the model.
- **Non-linearity**: Linear regression can't capture non-linear relationships.

## **Extensions**

1. **Polynomial Regression**: Models non-linear relationships by adding polynomial terms.
2. **Regularized Regression**: Techniques like Ridge and Lasso reduce overfitting by penalizing large coefficients.

## **Summary**

Linear regression is a powerful method for modeling relationships and making predictions. Key to its success are understanding its assumptions, selecting relevant predictors, and carefully evaluating the model's performance.

## **Glossary**

| **Term**                       | **Definition**                                                       |  
|---------------------------------|-----------------------------------------------------------------------|  
| Linear Regression               | Modeling the relationship between a dependent and independent variable. |  
| Predictor                       | An independent variable used for prediction.                         |  
| Response                        | The dependent variable being predicted.                              |  
| Residuals                       | The difference between observed and predicted values.                |  
| \( R^2 \)                       | A measure of how well the model explains the variance in the response. |  
| Adjusted \( R^2 \)              | A version of \( R^2 \) adjusted for the number of predictors.        |  
| Multicollinearity               | High correlation between predictors.                                  |  
| VIF                             | A metric for detecting multicollinearity.                             |  
| Regularized Regression          | Techniques like Ridge and Lasso to prevent overfitting.               |
